{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wed-IDCZl2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8745bb9a-0c8a-4c6f-cb5d-c842196d0a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrvaeJVLZyGA"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers accelerate evaluate rouge_score\n",
        "!pip install transformers accelerate evaluate rouge_score\n",
        "!pip install sacrebleu\n",
        "!pip install meteor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VTyfu9hZ5zj"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "\n",
        "import evaluate\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "from transformers import LineByLineTextDataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlkzCM-jww1_"
      },
      "outputs": [],
      "source": [
        "train_file_path = \"/content/drive/MyDrive/AESLC-master/EmailSubjectTrain.txt\"\n",
        "eval_file_path = \"/content/drive/MyDrive/AESLC-master/EmailSubjectEval.txt\"\n",
        "model_name = 'gpt2'\n",
        "rouge = evaluate.load('rouge')\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "#meteor = evaluate.load('meteor')\n",
        "output_dir = '/content/drive/MyDrive/AESLC-master/'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 1\n",
        "num_train_epochs = 1\n",
        "save_steps = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq3RFUNZZ8hQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d72d5c-acef-4fd4-be46-9c1bc9ebab7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "list_of_email = []\n",
        "list_of_subject = []\n",
        "list_of_ann0 = []\n",
        "list_of_ann1 = []\n",
        "list_of_ann2 = []\n",
        "list_of_references = []\n",
        "with open(eval_file_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        file_content = line.replace(\"\\t\", \" \").replace(\"\\n\", \" \").split(\"Subject :\")\n",
        "        word = file_content[0]\n",
        "#         print(line)\n",
        "        if len(word.split()) <= 400 :\n",
        "            try:\n",
        "              subject = file_content[1].split(\"ann0 :\")[0]\n",
        "              ann0 = file_content[1].split(\"ann0 :\")[1].split(\"ann1 :\")[0]\n",
        "              ann1 = file_content[1].split(\"ann0 :\")[1].split(\"ann1 :\")[1].split(\"ann2 :\")[0]\n",
        "              ann2 = file_content[1].split(\"ann0 :\")[1].split(\"ann1 :\")[1].split(\"ann2 :\")[1]\n",
        "              list_of_email.append(file_content[0])\n",
        "              list_of_references.append([subject, ann0, ann1, ann2])\n",
        "            except :\n",
        "              print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5APCETp6xZQ0"
      },
      "outputs": [],
      "source": [
        "def load_dataset(file_path, tokenizer):\n",
        "    dataset = LineByLineTextDataset(\n",
        "                tokenizer=tokenizer,\n",
        "                file_path=file_path,\n",
        "                block_size=512\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm\n",
        "    )\n",
        "\n",
        "    return data_collator\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    # TODO: Separate only the subject from string\n",
        "    # Ensure that for preds, you have a list of only the generated subject parts\n",
        "    # For labels, it should be a list of list of only the reference subjects\n",
        "    # NO OTHER CONTENT: EMAIL / SEPARATORS SHOULD BE OUTPUT AFTER POSTPROCESSING\n",
        "\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    \"\"\"\n",
        "    Original Trainer may have a memory leak.\n",
        "    This is a workaround to avoid storing too many tensors that are not needed.\n",
        "    \"\"\"\n",
        "    # print('logits:', logits.shape)\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    # print('pred_ids:', pred_ids.shape)\n",
        "\n",
        "    return pred_ids, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    i = 0\n",
        "    size = len(list_of_email)\n",
        "    list_of_prediction = []\n",
        "    while i < size:\n",
        "#               print(i)\n",
        "#               print(list_of_references[i])\n",
        "              inputs = tokenizer(list_of_email[i] + ' Subject : ', return_tensors=\"pt\")\n",
        "              inputs['input_ids'] = inputs['input_ids'].cpu()  # Move input tensor to CPU if necessary\n",
        "              device = torch.device(\"cuda:0\")  # Specify the CUDA device\n",
        "              model.to(device)  # Move the model to the CUDA device\n",
        "\n",
        "              # Move the input tensor to the CUDA device\n",
        "              inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "              outputs = model.generate(inputs['input_ids'], max_new_tokens=5, do_sample=True, top_k=30, top_p=0.95)\n",
        "              prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "              # Generate outputs using the model on the CUDA device\n",
        "              #print(prediction)\n",
        "              prediction = prediction.split('Subject : ')[1]\n",
        "              list_of_prediction.append(prediction)\n",
        "#               print(prediction)\n",
        "              i = i + 1\n",
        "    result = rouge.compute(predictions=list_of_prediction, references=list_of_references)\n",
        "    results_sacrebleu = sacrebleu.compute(predictions=list_of_prediction, references=list_of_references, lowercase = True)\n",
        "    #results_meteor = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\n",
        "        \"R1\": round(result[\"rouge1\"], 4),\n",
        "        \"R2\": round(result[\"rouge2\"], 4),\n",
        "        \"RL\": round(result[\"rougeL\"], 4),\n",
        "        \"RLsum\": round(result[\"rougeLsum\"], 4),\n",
        "        \"bleu\": round(results_sacrebleu[\"score\"], 4)\n",
        "    }\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.save_pretrained(output_dir)\n",
        "\n",
        "train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "eval_dataset = load_dataset(eval_file_path, tokenizer)\n",
        "data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "#           evaluation_strategy = \"epoch\",\n",
        "#           eval_steps = 500,\n",
        "          learning_rate=1e-5,\n",
        "          save_strategy = \"epoch\",\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          per_device_eval_batch_size=1,\n",
        "          num_train_epochs=num_train_epochs\n",
        "      )\n",
        "\n",
        "trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "#           eval_dataset=eval_dataset,\n",
        "#           preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "#           compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}